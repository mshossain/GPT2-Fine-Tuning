{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2kEFeagNSNyF",
   "metadata": {
    "id": "2kEFeagNSNyF"
   },
   "source": [
    "**Author:** Dr. Shahriar Hossain <br>\n",
    "**Topic of the code:** Fine-tuning GPT2 and retrieving embedding vectors from it <br>\n",
    "**Video explaining this code:**  <br>\n",
    "Part 1: https://youtu.be/2bqjzUX9ssE <br>\n",
    "Part 2: https://youtu.be/ZogRxshfWOQ <br>\n",
    "**My YT Channel:** https://www.youtube.com/@C4A <br>\n",
    "**Web:** https://courses.computing4all.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b356f6-b2f2-420a-bc64-8fbac98cbca2",
   "metadata": {
    "id": "d4b356f6-b2f2-420a-bc64-8fbac98cbca2",
    "outputId": "d150fa2c-d6e8-48e0-81e7-c859f74ba3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ca03fb-c8d6-4a30-ab48-dbc05f4827e9",
   "metadata": {
    "id": "45ca03fb-c8d6-4a30-ab48-dbc05f4827e9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## Keep your training documents in a folder named 'data'\n",
    "data_dir = \"data\"\n",
    "output_file = \"all_data.txt\"\n",
    "\n",
    "def is_hidden(filepath):\n",
    "    return os.path.basename(filepath).startswith('.')\n",
    "\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for filename in os.listdir(data_dir):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if not is_hidden(filepath):\n",
    "            with open(filepath) as infile:\n",
    "                for line in infile:\n",
    "                    # only write the line if it's not empty\n",
    "                    # (and, not just whitespace)\n",
    "                    if line.strip():\n",
    "                        outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf71a0a-86c5-457d-8de8-d148f10a453e",
   "metadata": {
    "id": "abf71a0a-86c5-457d-8de8-d148f10a453e",
    "outputId": "47cc5ce2-77a6-4bc8-a3ed-f7f530844f64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "C:\\Users\\mshos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.079500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=54, training_loss=0.4986020045148002, metrics={'train_runtime': 4.2672, 'train_samples_per_second': 25.309, 'train_steps_per_second': 12.655, 'total_flos': 7054884864000.0, 'train_loss': 0.4986020045148002, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, \\\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## GPT-2 Small ('gpt2'): 124 million parameters.\n",
    "## GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n",
    "## GPT-2 Large ('gpt2-large'): 774 million parameters.\n",
    "## GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Your custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.text = f.read().splitlines()\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            self.text[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\")\n",
    "        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "# Load data\n",
    "data = CustomDataset(tokenizer, \"all_data.txt\", 128)\n",
    "\n",
    "# Create a data collator that will dynamically pad the sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3, # Increse for more training from the fine-tuning data\n",
    "    learning_rate=1e-4,  # Decrease the learning rate for smaller fine-tuning data\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    evaluation_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    "    eval_dataset=None,  # You can specify an evaluation dataset here\n",
    "    data_collator=data_collator,  # Add the data collator here\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e299f232-ddc9-4e13-9d85-8f5fe7431107",
   "metadata": {
    "id": "e299f232-ddc9-4e13-9d85-8f5fe7431107",
    "outputId": "c0ad29d5-aae7-43ea-f994-24d0bb1ac5d3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the cat and the dog play together.\n"
     ]
    }
   ],
   "source": [
    "# Ensure your model is in evaluation mode\n",
    "# to disable dropout layers\n",
    "model.eval()\n",
    "\n",
    "# Create a prompt text for the model to complete\n",
    "prompt_text = \"Where is the cat\"\n",
    "\n",
    "# Tokenize the prompt text and convert to tensor\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "attention_mask = tokenizer(\n",
    "    prompt_text, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "# Move input_ids and attention_mask tensor to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    temperature=1.5,\n",
    "    top_k=50,\n",
    "    do_sample=True  # Enable sampling to consider temperature setting\n",
    ")\n",
    "\n",
    "# Decode the generated text back to string\n",
    "generated_text = tokenizer.decode(output[0],\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff4f08e-e81e-4835-8735-b199f4d2aa9e",
   "metadata": {
    "id": "8ff4f08e-e81e-4835-8735-b199f4d2aa9e"
   },
   "outputs": [],
   "source": [
    "## Retireve embeddings\n",
    "input_text= \"cat on the mat\"\n",
    "input_tokens = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Ensure tokens are on the same device as the model\n",
    "input_tokens = {k: v.to(device) for k, v in input_tokens.items()}\n",
    "\n",
    "# Forward pass, get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_tokens, output_hidden_states=True)\n",
    "\n",
    "# Only take the hidden states (ignore other outputs)\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "## If you want the embeddings from the last layer of the model:\n",
    "last_layer_embeddings = hidden_states[-1]\n",
    "\n",
    "## the last_layer_embeddings tensor obtained from the\n",
    "# GPT-2 model's forward method is 3D\n",
    "\n",
    "# Mean pool the last_layer_embeddings (across the sequence length dimension)\n",
    "mean_pooled = last_layer_embeddings.mean(dim=1)\n",
    "\n",
    "mean_pooled_embedding =  mean_pooled.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a822267-d20d-473f-87b5-8cdb77a39525",
   "metadata": {
    "id": "2a822267-d20d-473f-87b5-8cdb77a39525",
    "outputId": "0367ee51-5c13-4f25-817a-13f166dbb7f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3135e-01,  2.1256e-02, -1.0330e+00, -2.7195e-02, -2.2209e-01,\n",
      "        -1.6953e-01,  1.4524e-01,  1.7413e-01, -2.0055e-01, -1.3647e-01,\n",
      "         8.0561e-01, -1.6939e-01,  1.7410e-01, -1.0186e-01, -3.7826e-01,\n",
      "         4.0437e-01,  6.4153e-01,  1.8059e-01,  1.6409e-01,  3.8082e-01,\n",
      "         1.6841e-01,  1.2776e-01, -4.6799e-01,  5.0627e-01,  1.2089e-01,\n",
      "         3.4201e-01,  4.3682e-01, -7.3357e-02, -4.7435e-01,  3.7694e-01,\n",
      "         1.2478e-01, -2.8324e-01, -1.7686e-01, -6.3647e-03, -4.0675e-01,\n",
      "         7.4542e-01,  5.4965e+01, -8.5667e-02, -4.0177e-01,  4.9649e-01,\n",
      "        -3.1704e-01,  1.4997e-01, -1.0412e-02, -3.2140e-01,  1.4524e-01,\n",
      "        -6.7912e-02,  2.8797e-01,  5.7490e-01, -7.2651e-01,  1.2695e-01,\n",
      "        -1.1686e-01, -1.6036e-01,  2.2108e-01,  1.0030e-01,  7.0536e-01,\n",
      "         1.5738e+00,  1.1705e-01, -3.0475e-01,  2.9797e-02,  2.5755e-01,\n",
      "         1.7395e-01, -2.6267e-02,  3.3774e-01, -4.3090e-02, -1.0916e+00,\n",
      "        -9.5211e-02, -4.2127e-02,  1.3542e-01, -8.9189e-01,  3.3132e-01,\n",
      "         4.1233e-01, -3.1005e-01,  2.5020e-01,  3.1607e-01, -8.8349e-02,\n",
      "        -4.8851e-01,  1.2402e-01, -3.0284e-01,  3.3263e-01, -1.5793e-04,\n",
      "        -3.5042e-01,  3.3611e-02,  1.9559e-01,  4.2996e-01, -4.8855e-01,\n",
      "        -8.9233e-02, -7.7360e-01, -1.0143e+00, -2.3960e-01, -2.0069e-01,\n",
      "        -4.5837e-02,  2.5832e-01, -9.7263e-01, -8.5743e-02, -4.4733e-01,\n",
      "        -1.5352e-01,  6.8517e-01, -7.4063e-01, -6.9528e-02,  2.6637e-01,\n",
      "        -7.2487e-01,  1.7358e-01, -3.5936e-03, -6.6725e-01, -5.2999e-01,\n",
      "         7.3995e-02, -1.0300e-01,  1.9563e+00,  1.0356e+00, -6.7837e-02,\n",
      "         1.9753e-01, -5.8261e-01, -2.7565e-02,  2.6798e-01,  3.5176e-01,\n",
      "         3.3857e-01,  2.2498e-01, -3.5880e-01, -9.2664e-02,  3.9863e-01,\n",
      "        -3.2957e-01,  5.0080e-01, -1.4062e-01,  1.4394e-02, -3.4407e-02,\n",
      "        -4.3349e-02, -4.2091e-01, -2.4764e-01, -1.7069e-01, -4.7023e-02,\n",
      "        -1.9919e-01, -4.7262e-01, -8.4770e-02,  3.3479e-02, -1.2014e-01,\n",
      "         4.0037e-01,  1.0116e-01, -6.8177e-01,  5.2329e-01,  2.2691e-01,\n",
      "         3.2404e-01,  3.1227e-01,  3.1557e-01,  3.6621e-01,  3.8423e-01,\n",
      "        -4.4176e-01, -1.7018e-01, -1.7804e-01, -1.5002e-01, -2.3666e-01,\n",
      "        -7.7405e-01,  6.3437e-01,  2.8398e-01, -3.0112e-02, -5.3653e-02,\n",
      "         3.0876e-03, -5.7197e-01, -3.4271e-01,  4.1650e-01, -7.5533e-02,\n",
      "        -8.7495e-01,  2.0511e-01, -1.2624e-01, -8.5671e-02, -1.5526e-01,\n",
      "         5.9032e-01,  2.9554e-01,  8.4278e-02, -3.6463e-01,  2.1374e-02,\n",
      "        -7.4889e-01, -2.7033e-01, -2.0742e-01, -7.8201e-01,  8.8215e-02,\n",
      "        -5.4996e-01, -3.6759e-01,  6.9299e-02, -1.7797e-01, -2.6437e-01,\n",
      "        -1.5304e-01,  1.8934e-01,  4.3262e-01,  1.1615e-01,  1.8510e-01,\n",
      "        -3.2225e-01, -2.3337e-01,  3.8601e-01,  3.4076e-01, -1.1462e-01,\n",
      "        -3.2394e-01, -2.4194e-01, -5.4386e-02, -1.2957e-01,  1.5580e-03,\n",
      "        -1.2217e+00,  4.2745e-01, -7.3512e-01,  1.3081e-01, -1.0406e+00,\n",
      "        -1.8158e-01,  3.3709e-01, -4.5197e-01,  8.7921e-02,  2.1550e-01,\n",
      "        -1.8906e-01, -1.7488e-01, -3.1716e-01, -7.0798e-01, -6.0836e-01,\n",
      "        -1.0024e-01,  1.3425e-02,  2.7427e-01, -6.2667e-01,  1.5958e-01,\n",
      "        -6.4493e-02, -2.0708e-01, -2.5081e-01,  2.3657e-01, -9.6253e-02,\n",
      "         1.4636e-01,  5.3301e-01,  1.3275e-01,  1.7994e-01, -9.2550e-02,\n",
      "        -7.3748e-01, -3.2135e-02,  4.2056e-01, -3.3938e-01,  1.0948e-01,\n",
      "         1.3131e-01, -2.8912e-01,  7.5944e-01, -4.8242e-01,  2.7558e-01,\n",
      "        -2.8334e-01, -2.0484e-01, -2.4643e-01,  4.1044e-01, -4.5871e-01,\n",
      "        -5.4889e-01,  1.8231e-01,  1.8443e-01,  7.1495e-02, -2.2880e-01,\n",
      "         3.9556e-02, -4.9531e-02, -2.4959e-01, -2.7154e-01,  1.7930e-01,\n",
      "         4.3102e-01,  2.6730e-02,  2.2534e-01,  2.7162e-01, -5.3302e-01,\n",
      "        -7.1153e-02, -1.8175e+00, -3.1568e-01, -1.9832e-01, -1.6297e-01,\n",
      "         1.4518e-01,  5.3659e-01,  5.3018e-02,  2.6212e-01, -1.9850e-01,\n",
      "         3.4377e-01, -1.0039e+00,  3.7739e-01, -1.5517e-01,  3.8956e-01,\n",
      "        -1.0876e-01,  2.1957e-01, -6.7218e-01, -1.6526e-01,  7.0965e-02,\n",
      "        -7.7528e-02,  6.8226e-01, -3.6850e-02, -4.4219e-01,  2.7731e-01,\n",
      "         6.6990e-02,  2.5255e-01,  4.9517e-01, -6.1131e-02,  3.4529e-01,\n",
      "        -8.0621e-02, -1.2478e-01,  5.5619e-01,  3.9834e-01,  9.4979e-01,\n",
      "         2.8356e-01, -2.2303e-01,  2.5319e-01,  8.2031e-01, -3.7182e-02,\n",
      "        -2.8203e-01, -1.4222e-01, -1.2587e-01,  5.2648e-01,  1.3632e-01,\n",
      "        -3.9585e-01, -1.0869e-01, -2.1751e-01,  4.0558e-02, -4.0079e-01,\n",
      "        -2.0951e-01,  4.4419e-02,  7.7416e-02, -4.8159e-01, -1.3168e-01,\n",
      "         2.4928e-01,  1.5961e-01, -2.4044e-01, -2.8011e-01, -2.9647e+01,\n",
      "        -3.3162e-01, -2.6743e-01,  1.9406e-01, -1.9161e-01,  3.1758e-02,\n",
      "        -1.0176e+00,  1.2961e-01,  3.1772e-01,  2.5259e-01, -5.2068e-02,\n",
      "        -2.4535e-02,  2.8961e-01,  3.2883e-01, -1.3346e-01, -2.4992e-01,\n",
      "         2.5325e-01,  1.2920e-01, -1.3643e-01, -2.5390e-01,  5.3830e-01,\n",
      "         1.7155e-01, -5.5570e-01, -2.7603e-01, -7.3940e-01,  1.2313e-01,\n",
      "         6.4711e-02, -5.0610e-01,  2.7219e-01,  3.1708e-02, -3.2544e-01,\n",
      "         1.3862e-01, -3.6920e-01,  2.2042e-01, -2.4590e-01, -2.2690e-01,\n",
      "         1.2862e-01,  4.1512e-01,  3.6673e-02, -5.9392e-02,  2.0746e-01,\n",
      "         1.1927e-01,  4.0238e-01, -5.3044e-01,  3.5616e-01, -1.9508e-01,\n",
      "        -4.5565e-01,  1.0408e+00,  2.5872e+00,  2.4141e-01, -4.8084e-01,\n",
      "        -4.3737e-01, -7.1403e-02, -7.6208e-01, -5.2094e-01,  2.5489e-01,\n",
      "         7.1745e-01,  3.3557e-01, -2.5245e-01, -1.0992e+00, -7.6422e+00,\n",
      "        -1.3788e-01, -1.5676e-01,  5.3752e-01, -2.1010e-01, -1.2258e-01,\n",
      "        -1.6857e-01, -2.8791e-01,  1.6924e-01,  8.9096e-01, -8.2961e-02,\n",
      "        -1.5981e-01,  3.9989e-01, -1.6162e-01,  2.8232e-01, -1.0477e+00,\n",
      "         4.1929e-01, -9.5083e-03, -5.5961e-01,  3.9015e-02, -1.1022e-01,\n",
      "         4.5811e-01, -8.1222e-01,  5.3410e-02, -8.9112e-02, -2.1137e-01,\n",
      "         3.1313e-02,  8.4011e-02,  9.9968e-02, -5.1330e-01,  4.3925e-01,\n",
      "         3.7998e-01,  7.2584e-02, -5.2456e-01,  8.4182e+00, -4.2401e-02,\n",
      "        -2.8002e-01, -7.7057e-02, -3.9142e-01, -2.2579e-01,  5.0212e-01,\n",
      "         5.1003e-02,  2.9387e-01,  1.0388e-01,  1.8102e-01,  4.8831e-01,\n",
      "         6.3448e-01, -7.5350e-01,  2.2800e-01,  3.2030e-01,  7.5227e-02,\n",
      "        -2.4903e-01,  4.5505e-01,  1.7796e-01,  6.2455e-02, -4.2330e-01,\n",
      "         5.9005e+01,  1.5467e-01,  4.0457e-01,  7.7219e-01, -6.0075e-01,\n",
      "         1.2594e-01,  9.9374e-02,  3.7691e-01, -3.2488e-01, -2.4767e-01,\n",
      "        -1.3270e-01,  3.3429e-02,  1.9752e+00, -5.6522e-01,  2.6233e-01,\n",
      "         5.2197e-01, -2.9867e-01,  1.1371e+00,  1.9292e-01,  3.7679e-01,\n",
      "         2.9728e-01,  4.8288e-02,  1.1847e-01, -1.0174e+00,  5.2232e-01,\n",
      "         9.7087e-01, -1.8641e-01, -2.2903e-01,  1.9724e-01,  2.4863e-01,\n",
      "         1.2556e-01, -5.6424e-01,  1.6554e-01, -7.5938e-01,  7.3532e-02,\n",
      "        -3.2970e-01, -5.7487e-01,  1.2044e-01,  4.0963e-02,  5.9017e-01,\n",
      "        -2.7972e-01, -2.0093e-02, -3.0682e-01,  1.0703e-01, -6.4000e-01,\n",
      "        -3.8262e-01,  5.3895e-01, -2.3412e-01, -6.1153e-01, -1.2018e+01,\n",
      "        -8.8155e-01,  1.0900e+00,  3.1333e-02, -3.6842e-02,  8.1231e-02,\n",
      "         6.3811e-03, -3.6672e-02, -2.8399e-01, -1.2625e-01,  2.8922e-03,\n",
      "        -3.0443e-01,  1.6667e-01, -7.9502e-02, -4.7839e-01, -6.6737e-01,\n",
      "        -2.3629e-01,  1.3230e+02, -6.8025e-01, -3.4349e-01, -6.7055e-02,\n",
      "         3.0263e-01, -4.2299e-01,  3.4935e-01, -1.2209e+00, -2.1731e-01,\n",
      "         3.0443e-01,  2.1123e-01, -1.0109e-01, -1.4879e-01,  2.3359e-01,\n",
      "         1.3939e-01,  8.7894e-01, -4.0634e-02, -9.8745e-02,  2.5592e-01,\n",
      "        -6.1008e-01, -4.1035e-01,  6.0833e-01,  1.5702e-01, -5.5915e-02,\n",
      "         2.2217e-01, -1.7122e-01,  3.4538e-01,  7.4408e-02, -2.6570e-01,\n",
      "        -5.6099e-01,  2.1160e-01, -5.5226e-01,  2.1495e-01,  1.6062e-01,\n",
      "         5.8601e-02, -1.9533e-01, -1.2295e-01, -6.1026e-02, -9.5387e-01,\n",
      "         1.4009e-01, -6.4637e-01,  7.1356e-01,  2.4308e-01,  1.6955e-01,\n",
      "         1.0514e-02, -3.3255e-01, -2.4191e-01, -5.2456e-01,  2.4974e-01,\n",
      "         5.2004e-03,  3.7290e-01,  4.9755e-01,  1.5839e-01, -6.6362e-02,\n",
      "         2.3215e-01, -1.7843e-01,  2.9157e-01,  1.6947e-01, -1.8046e+00,\n",
      "         1.6297e-02, -3.6837e-01,  3.0560e-01,  1.4448e-01,  2.9975e-02,\n",
      "        -2.5746e-01, -4.4137e-01,  5.8313e-01, -4.9449e-01,  4.9024e-01,\n",
      "        -1.4622e-01,  7.7182e-02, -1.4123e-01, -4.5247e-02,  4.6909e-02,\n",
      "         4.3923e-01, -1.8808e-01,  3.0685e-01, -2.3282e-01,  1.6270e-01,\n",
      "         3.5506e-01, -4.0714e-02,  5.4618e-02,  5.1130e-04,  4.7114e-02,\n",
      "        -3.5484e-01,  2.3258e-01, -4.7273e-01,  9.0702e-03, -3.3406e-01,\n",
      "        -3.9368e-01,  6.9504e-01,  2.2840e-01, -4.1874e-01,  4.6485e-01,\n",
      "         2.3747e-01,  5.3615e-01,  5.8561e-03,  2.6907e-01,  8.9153e-02,\n",
      "        -1.8105e-01,  1.7516e-01,  2.0529e-01,  2.7141e-01,  3.5725e-01,\n",
      "        -7.6398e-01, -5.0387e-01, -6.8691e-01, -1.1366e-03,  3.2637e-01,\n",
      "         5.7651e-02,  2.7788e-01, -3.1674e-01,  3.2206e-01, -1.7772e-01,\n",
      "         1.6180e-01,  1.4801e-01, -3.5406e-02,  5.9903e-01,  2.9923e-01,\n",
      "        -9.8950e-02,  9.6156e-03,  1.9848e-01, -1.6612e-01,  2.2112e-01,\n",
      "         1.7974e-01,  2.8976e-01, -1.7854e-01,  6.8356e-01,  2.5884e-01,\n",
      "        -1.7226e-01, -2.3853e-01, -1.4711e+00, -1.0461e-01, -1.5063e-01,\n",
      "         2.4099e-01, -1.1236e-01,  7.6401e-02,  7.8253e-02, -4.2695e-01,\n",
      "        -1.3076e-01, -1.9462e-01,  2.3412e-01, -4.3782e-01, -4.4996e-01,\n",
      "        -1.7001e-01,  1.6472e-01, -3.8708e-01,  1.2721e-01,  1.3213e-02,\n",
      "        -2.9398e-02, -3.8664e-01,  1.4574e-01,  3.7997e-02, -8.7648e-02,\n",
      "        -8.1368e-01, -1.1041e-01,  4.9662e-01, -1.0699e-01,  3.4250e-01,\n",
      "         3.3159e-01,  9.8626e-02, -1.8531e-01, -2.3457e-01,  5.3641e-01,\n",
      "         2.3121e-01,  3.1924e-01, -8.6372e-02,  5.5468e-01,  1.3514e-01,\n",
      "         1.4445e-01, -2.6677e-02, -2.5458e-01,  1.5405e+00,  3.0092e-01,\n",
      "        -1.6088e+00,  8.5762e-02, -2.0095e-01,  1.5361e-01, -2.5427e-01,\n",
      "         2.4716e-02, -2.6775e-01, -6.8469e-02,  7.9499e-01, -3.3093e-01,\n",
      "        -8.8575e-01,  3.0615e-01, -1.4282e-01, -2.1784e-03, -2.6891e-01,\n",
      "        -1.5319e-01,  7.5811e-02, -5.3166e-01, -4.2584e-01, -1.0224e-01,\n",
      "        -3.1808e-01, -5.0065e-02, -2.5653e-01, -3.8444e-01, -6.7748e-01,\n",
      "         1.9123e-01,  2.8482e-01,  2.7600e-01, -1.4622e-01,  5.7086e-01,\n",
      "        -3.0733e-02,  2.7458e-01,  6.2134e-01,  3.9671e-02,  1.7072e-01,\n",
      "        -4.5411e-01, -1.2497e+00, -1.0465e-01,  4.3427e-02, -5.0559e-01,\n",
      "        -2.5148e-01, -1.4494e-01,  7.6357e-01, -4.0443e-01,  3.7567e-01,\n",
      "        -2.1687e-02,  3.2660e-01, -3.9698e-01,  2.4242e-01,  1.0379e-01,\n",
      "        -5.3996e-01,  1.1428e-01, -3.2347e-01,  7.1108e-02,  1.5299e-01,\n",
      "         1.2361e-01,  1.1609e+00, -7.2265e-02, -1.7833e-01,  3.5373e-01,\n",
      "         4.7758e-01,  1.5424e-01, -2.5401e-01, -2.6056e-01,  1.9228e-01,\n",
      "         5.5319e-02,  1.6220e-01,  4.0167e-01,  2.7928e-01, -4.9933e-01,\n",
      "        -3.4204e-01, -3.3382e-01,  2.6724e-01, -5.3938e-01, -4.0961e-01,\n",
      "        -4.4323e-01,  2.6126e-01, -2.9014e-01, -4.7039e-01, -4.8219e-02,\n",
      "        -4.4978e-01,  6.2888e-01, -5.3307e-01,  3.0794e-01,  4.5277e-02,\n",
      "        -3.5425e-02, -4.6103e-01,  1.1923e-01, -3.2991e-01, -3.8884e-01,\n",
      "         8.4805e-02, -1.2623e-01,  8.7305e+00, -4.9092e-03,  5.7530e-01,\n",
      "         1.0869e-02,  4.4766e-01, -1.4309e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(mean_pooled_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b70f387b-5668-4f36-9cf7-27caf98e4b38",
   "metadata": {
    "id": "b70f387b-5668-4f36-9cf7-27caf98e4b38",
    "outputId": "bfb7d479-c4d2-47d7-a7c0-c5419a3b38f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(mean_pooled_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef6683-3c8f-4300-965d-5d65b4059010",
   "metadata": {
    "id": "a7ef6683-3c8f-4300-965d-5d65b4059010"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f782f33-54a0-43ac-9984-718ec79e3367",
   "metadata": {
    "id": "6f782f33-54a0-43ac-9984-718ec79e3367"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
